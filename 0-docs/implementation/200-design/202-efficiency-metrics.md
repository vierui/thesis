# Comprehensive Efficiency Analysis Framework for CITI KMS

## 1. Key Performance Metrics to Track

### A. Computational Resource Metrics

**CPU Metrics:**  
- CPU utilization percentage (per service)  
- CPU time per request/operation  
- CPU cores usage distribution  
- Process-level CPU consumption  

**Memory Metrics:**  
- RAM usage (absolute MB/GB)  
- Memory allocation rate  
- Peak memory consumption  
- Memory leaks detection (growth over time)  
- Cache hit/miss ratios  

**GPU Metrics (if applicable for embeddings/LLM):**  
- GPU utilization %  
- VRAM usage  
- GPU memory bandwidth  
- Inference throughput (tokens/sec)  

**Storage I/O:**  
- Disk read/write operations per second (IOPS)  
- Disk throughput (MB/s)  
- Storage latency  
- SFTP transfer speeds  
- Database query I/O  

**Network Metrics:**  
- Network bandwidth usage (upload/download)  
- Request/response payload sizes  
- Connection pool utilization  
- API call latency  

### B. Pipeline Performance Metrics

**Latency Metrics:**  
- End-to-end response time  
- Per-component latency breakdown  
- Time-to-first-token (TTFT) for streaming  
- P50, P95, P99 latency percentiles  

**Throughput Metrics:**  
- Requests per second (RPS)  
- Documents processed per minute  
- Queries handled concurrently  
- Embedding generation rate (chunks/sec)  

**Quality vs. Cost Trade-offs:**  
- RAG evaluation scores (RAGAS metrics) vs. computational cost  
- Retrieval accuracy vs. search time  
- Answer quality vs. inference time  

---

## 2. Pipeline Measurement Points

### Document Ingestion Pipeline
User Upload â†’ SFTP Transfer â†’ Document Parsing â†’ Chunking â†’ Embedding â†’ Vector DB Insert  
Measurement Points:
- **Upload Transfer:** Network I/O, transfer time, file size  
- **SFTP Storage Operations:** Disk I/O, write latency  
- **Document Parsing:** CPU usage, parsing time per page, memory consumption  
- **Text Chunking:** CPU time, memory allocation, chunk generation overhead  
- **Embedding Generation:** GPU/CPU usage, inference time, batch size efficiency  
- **Vector DB Insertion:** Insert throughput, index building time, memory usage  

### Query/RAG Pipeline
User Query â†’ Query Embedding â†’ HyDE (optional) â†’ Vector Search â†’ Reranking (optional) â†’ LLM Generation â†’ Response Stream  
Measurement Points:
- **Query Reception:** Request queue length, concurrent connections  
- **Query Embedding:** Embedding API latency, CPU/GPU usage  
- **HyDE Expansion:** LLM inference time, tokens generated  
- **Vector Search:** Search latency, CPU/memory for index traversal  
- **Reranking:** Model inference time, precision improvement vs. latency cost  
- **LLM Generation:** TTFT, tokens/sec, total inference time  
- **Response Streaming:** Network throughput, SSE overhead  

---

## 3. Analysis Methodology

### Phase 1: Baseline Profiling
- Add instrumentation & monitoring (timing decorators, Prometheus, logs)  
- Create test datasets (small, medium, large docs)  
- Simulate realistic query patterns  
- Collect resource usage and latency data  

### Phase 2: Bottleneck Identification
- Create resource consumption matrix (CPU, memory, GPU, time, throughput)  
- Perform critical path analysis (T_embed, T_search, T_llm)  
- Test performance under scaling (documents, users, complexity)  

### Phase 3: Controlled Experiments
- **Batch Size Optimization:** Throughput vs. memory tradeoff  
- **Search Parameters:** Impact of `top_k` on latency and quality  
- **Chunking Strategy:** Granularity vs. embedding cost  
- **HyDE/Reranking ROI:** Cost vs. quality gain  
- **Parser Comparison:** Speed vs. extraction quality  
- **LLM Context Length:** Memory and latency vs. answer quality  

---

## 4. Expected Bottlenecks & Optimization Strategies

### ðŸ”´ Critical Bottlenecks (Highest Impact)

#### **1. LLM Inference Time**
- **Impact:**Â 50-70% of total latency
- **Resource:**Â GPU/CPU, memory
- **Current Config:**Â gpt-4-turbo, max_tokens=4096, temperature=0.01

**Optimization Strategies:**

- âœ…Â **Model Quantization:**Â Use INT8/INT4 quantized models (2-4x speedup, minimal quality loss)
- âœ…Â **Smaller Models:**Â Test Llama 3 8B â†’ Llama 3.2 3B/1B (3-5x faster, evaluate quality trade-off)
- âœ…Â **Speculative Decoding:**Â Draft tokens with small model, verify with large model
- âœ…Â **KV Cache Optimization:**Â Reuse key-value cache for multi-turn conversations
- âœ…Â **Batch Inference:**Â Process multiple queries together (if latency allows)
- âœ…Â **Reduce Max Tokens:**Â Analyze actual response lengths, reduce if possible
- âœ…Â **Early Stopping:**Â Implement dynamic stopping based on answer completeness

#### **2. Embedding Generation**

- **Impact:**Â 15-25% of document processing time, 5-10% of query time
- **Resource:**Â GPU/CPU, API latency
- **Current Config:**Â 1024-dim dense + sparse, external API

**Optimization Strategies:**

- âœ…Â **Local Embedding Models:**Â Deploy embedding model locally to eliminate network latency
- âœ…Â **Smaller Embeddings:**Â Test 768-dim or 384-dim models (faster, less storage)
- âœ…Â **Batch Optimization:**Â Increase batch size (current: 32) up to GPU memory limits
- âœ…Â **Caching:**Â Cache embeddings for frequently accessed queries
- âœ…Â **Async Processing:**Â Process document embeddings asynchronously
- âœ…Â **GPU Acceleration:**Â Ensure embeddings use GPU if available

#### **3. Vector Search (Hybrid)**

- **Impact:**Â 10-20% of query latency
- **Resource:**Â CPU, memory
- **Current Config:**Â Hybrid search (dense + sparse), top_k=60

**Optimization Strategies:**

- âœ…Â **Index Optimization:**Â Use HNSW or IVF_FLAT instead of FLAT for large collections
- âœ…Â **Reduce top_k:**Â Experiment with lower values (20-40 may be sufficient)
- âœ…Â **Single Search Mode:**Â Test if hybrid is necessary, or if dense-only is sufficient
- âœ…Â **Collection Partitioning:**Â Partition by user/topic for faster searches
- âœ…Â **Query Result Caching:**Â Cache search results for popular queries
- âœ…Â **Load Balancing:**Â Distribute searches across Milvus replicas
### ðŸŸ¡ Moderate Bottlenecks

#### **4. Document Parsing**
- **Impact:**Â Variable (10-40% of ingestion time, depending on parser)
- **Resource:**Â CPU, memory
- **Current Config:**Â Supports PyMuPDF, Docling, MinerU

**Optimization Strategies:**
- âœ…Â **Parser Selection:**Â Use PyMuPDF for simple PDFs (fastest), Docling/MinerU only for complex layouts
- âœ…Â **Parallel Processing:**Â Process pages in parallel
- âœ…Â **Pre-processing Pipeline:**Â Extract text once, reuse for multiple purposes
- âœ…Â **OCR Optimization:**Â Use faster OCR models (e.g., Tesseract â†’ EasyOCR â†’ PaddleOCR comparison)

#### **5. Reranking (Optional)**
- **Impact:**Â 5-15% of query latency (when enabled)
- **Resource:**Â GPU/CPU

**Optimization Strategies:**

- âœ…Â **Conditional Reranking:**Â Only rerank when confidence is low
- âœ…Â **Lightweight Models:**Â Use smaller reranking models (e.g., MiniLM)
- âœ…Â **Reduce Candidates:**Â Rerank only top-20 instead of top-60

#### **6. HyDE (Optional)**

- **Impact:**Â 10-30% additional latency (when enabled)
- **Resource:**Â GPU/CPU, LLM API

**Optimization Strategies:**
- âœ…Â **Selective HyDE:**Â Only use for ambiguous/complex queries
- âœ…Â **Cache Expansions:**Â Cache HyDE outputs for similar queries
- âœ…Â **Smaller Model:**Â Use dedicated small model for HyDE
### ðŸŸ¢ Minor Bottlenecks

#### **7. SFTP File Transfer**

- **Impact:**Â <5% for most files, higher for very large files (>100MB)
- **Resource:**Â Network, disk I/O

**Optimization Strategies:**

- âœ…Â **Compression:**Â Enable compression during transfer
- âœ…Â **Parallel Uploads:**Â Allow multiple concurrent uploads
- âœ…Â **Local Caching:**Â Cache frequently accessed files

#### **8. Database Operations (PostgreSQL)**

- **Impact:**Â <5% typically
- **Resource:**Â CPU, disk I/O

**Optimization Strategies:**

- âœ…Â **Connection Pooling:**Â Ensure proper connection pool configuration
- âœ…Â **Query Optimization:**Â Index frequently queried fields
- âœ…Â **Batch Operations:**Â Batch inserts/updates where possible
---

## 5. Monitoring Dashboard Setup

**Stack:** Prometheus + Grafana, OpenTelemetry, ELK Stack  

**Dashboards:**  
- Real-time resource usage  
- Pipeline latency and throughput  
- Cost vs. quality  
- Alert system (high latency, CPU, memory, error rate)  

---

## 6. Implementation Roadmap (not applicable)

- **Week 1-2:** Instrumentation  
- **Week 3-4:** Baseline Measurements  
- **Week 5-6:** Run Experiments  
- **Week 7-8:** Optimize + Deploy + Validate  

---

## 7. Expected Impact

| Optimization                | Latency â†“ | Cost â†“ | Quality Î” |
|----------------------------|-----------|--------|------------|
| Model quantization (INT8)  | 40-60%    | 50-70% | -2 to -5%  |
| Smaller LLM (8B â†’ 3B)      | 60-75%    | 60-80% | -5 to -15% |
| Reduce top_k               | 5-10%     | 2-5%   | -0 to -3%  |
| Local embeddings           | 30-50%    | 40-60% | 0%         |
| Disable HyDE               | 15-25%    | 10-20% | -5 to -10% |
| Index optimization (HNSW)  | 20-40%    | 10-20% | 0%         |
| Chunk size (512â†’1024)      | 20-30%    | 30-40% | Â±5%        |

Goal: 50-70% latency & 40-60% cost reduction while maintaining >90% quality

---

## 8. Key Success Metrics

- Avg. query latency < **2s**  
- P95 latency < **4s**  
- Throughput > **10 QPS**  
- RAGAS scores > **0.8**  
- Cost/query â†“ in $ or resource hours  
- User satisfaction tracking (feedback frontend)

---

## âœ… Recommended First Steps

- Start with **instrumentation** and **baseline profiling**  
- Target **LLM + embedding** stages for max gains  
- Validate improvements with **controlled experiments**  
- Track performance **vs. quality** using dashboards  

---
