Agentic Orchestration for Efficient RAG+LLM Pipeline

Engineering Best Practices for Efficiency (Task A)

Table 1. Key patterns to minimize VRAM, energy, and latency while preserving answer quality. Each row outlines the pattern, where it applies in our stack, when to use it, expected impact on J/turn, VRAM peak, and P95 latency, potential pitfalls, and a three-step implementation/verification guide.

Pattern	Where in our stack	Preconditions	Impact (J/turn, VRAM, P95)	Pitfalls	3-Step “How-to Verify”
Dynamic model routing (small→big)	Flask API layer – add a router before LLM calls	Multiple LLMs deployed (e.g., local 4B + optional larger model API); classifier or logic to pick model	Cuts average compute and cost by routing easy queries to the 4B model. Studies show up to 30–62% cost reduction and 40% latency reduction vs always using a big model ￼ ￼. Energy per turn drops since smaller models use fewer GPU joules. Peak VRAM usage stays low on GPU (big model only loaded/called if needed).	Misclassification can send a hard query to a weak model, hurting quality (one report saw ~6 point accuracy drop with a naive router) ￼. Maintaining two models increases system complexity.	1) Implement router: e.g. a lightweight classifier in Flask that labels queries “simple” or “hard.” 2) Route accordingly: call local R-4B for simple queries; call external GPT-4 (or a larger model endpoint) for hard queries. 3) Verify: log each routing decision and evaluate outcomes – check if answers from the 4B are correct when chosen (manual review or user feedback) and measure reduction in latency/cost per query compared to always using the big model.
Query difficulty classifier (predictive routing)	Flask API – as part of request preprocessing	Labeled data or heuristics to predict query complexity/required reasoning; a small classifier model (e.g. fine-tuned BERT/DistilBERT) available	Accurate difficulty prediction lets us route each query to only one LLM (no cascade), minimizing wasted work. Expected impact: significant cost/energy savings (e.g. one approach cut total LLM usage cost ~30% ￼) and lower latency for easy queries (no big model call). VRAM usage remains low except when big model is truly needed.	Training the classifier requires ground truth (e.g. which model would answer correctly) – noisy labels can degrade routing. Static classifiers may misjudge novel queries (distribution drift). A wrong guess can either waste resources (unneeded big model call) or hurt answer quality (small model fails) ￼.	1) Train classifier: Use past queries and outcomes (or simulate with GPT-4 vs R-4B on a sample set) to label queries by “which model needed.” Train a classifier (DistilBERT or similar) to predict this from the query text. 2) Integrate in Flask: before retrieval/LLM steps, run the classifier to choose a route (small vs big LLM). 3) Verify & tune: log the classifier’s choice and the eventual answer accuracy. Periodically evaluate routing accuracy against a baseline (e.g. always GPT-4) to ensure minimal quality regression; adjust the classifier threshold or retrain as needed if misroutes are observed.
Tool selection (agent decides retrieval or other tools)	Flask pipeline (and in prompt) – agent logic before LLM call	Multiple tools available (e.g., vector retrieval vs none, code executor for math, etc.); signals to decide if a tool is needed (query type or content)	Avoiding unnecessary tools saves tokens and time (e.g. skipping retrieval on trivial questions cuts prompt length and latency). Conversely, invoking the right tool only when needed improves quality without always incurring tool overhead. Expected impact: slightly lower VRAM use and latency for queries that skip tools (since fewer context tokens), and better J/turn when expensive tool calls (like external APIs) are avoided. A 2024 study’s Adaptive RAG router showed that skipping retrieval on low-complexity queries maintained answer accuracy while using simpler pipeline steps within the same latency as always-retrieving ￼.	Tool misuse: if the agent skips retrieval when it was actually needed, the 4B model might hallucinate answers. Conversely, calling tools unnecessarily wastes time (e.g. running a calculator for a question the LLM could easily answer). Complex tool logic can also add overhead or failure points.	1) Define rules or train classifier: e.g. if query lacks proper nouns or dates, treat as self-contained → no retrieval; if it’s a math question, use calculator tool, etc. 2) Integrate in pipeline: in Flask, before calling the LLM, decide which tools to invoke (for retrieval, that means whether to query Milvus or not; for other tools, whether to call them). Pass the LLM a prompt that includes tool outputs only as needed. 3) Verify behavior: log which tools were used for each query. Compare outcomes on a test set with and without the tool policy (e.g., did skipping retrieval still yield correct answers on simple factual questions?). Monitor any errors when a tool is skipped (sign of a false negative in tool selection).
Prompt/context budgeting (dynamic context length management)	Flask – during prompt assembly for LLM	Long documents or many retrieved chunks; queries that could return excessive context; LLM context window limits (e.g. 4k tokens)	Controls VRAM and latency by keeping prompts concise. For example, restricting retrieval to top-k passages or truncating context ensures we don’t send maximum tokens for every query. Fewer tokens → lower compute per turn (J/turn) and faster responses. This also prevents hitting context size limits that would require multiple LLM calls. Impact: P95 latency becomes more predictable (bounded by set prompt length). VRAM peak is reduced because the model doesn’t have to attend to extremely long inputs.	Aggressive trimming or summarizing context can omit relevant info, hurting answer quality or requiring more back-and-forth. Summarization itself uses LLM compute and may introduce inaccuracies. If budgeting isn’t adaptive, we might under-utilize the context window for complex queries (not providing enough supporting info).	1) Set a context budget policy: e.g., allow up to N tokens for retrieval context. If initial top-k results exceed this, either truncate extra passages or compress them (summarize or extract key facts). 2) Implement in prompt builder: after Milvus returns results, take only the top-ranked chunks that fit in ~N tokens (account for user question + prompt overhead). If needed, implement a summarization step for overflow – e.g., use the 4B model to summarize additional text, then include summary in prompt. 3) Verify with extremes: test with a very long document query – check that the prompt length is within limits and the answer still cites key sources correctly. Log the number of tokens in each prompt and response (Flask can compute this from the text) to ensure most queries stay well under the limit. Monitor if any queries trigger truncation and later lead to user follow-ups (potentially indicating missing info).
KV/prompt/result caching (reuse past computations)	Multiple points: embedding service, LLM server (vLLM), Flask API	Query repetition or overlapping prefixes; limited domain questions (FAQs); vLLM PagedAttention enabled for KV caching	KV cache: vLLM’s PagedAttention shares key/value memory across requests with common prefixes, boosting throughput and reducing memory overhead ￼ ￼. This lowers GPU compute per token when users ask similar things (especially in multi-turn chats where the prompt prefix repeats). Prompt/embedding cache: avoid re-embedding identical text or re-querying Milvus for recent queries – saves CPU and retrieval latency. Result cache: if the exact same question was answered recently, directly return the cached answer (near-zero cost/latency). Overall impact: fewer redundant computations = lower average J/turn and CPU load; lower P95 latency for repeated queries (cache hits return instantly).	Cache invalidation and memory use. Storing too many results can consume RAM; stale caches may return outdated answers if underlying data changed. Over-caching can also degrade quality (e.g., returning an old answer that doesn’t exactly match a slightly rephrased question). Prefix KV caching helps only if requests align on prefixes – unpredictable in open-domain queries. Also, cache lookups add overhead for completely unique queries (which might be the majority).	1) Enable vLLM prefix caching: this is built-in with PagedAttention; ensure the LLM server is configured (e.g., gpu-memory-utilization flag) to maximize KV sharing ￼. Verify by checking vLLM logs/metrics (look for kv cache hits or that kv_pages_evicted stays low under load). 2) Implement query result caching in Flask: use a hash or exact-match on incoming questions. If seen before, skip LLM and return stored answer+sources (possibly flag it as cached for transparency). Maintain a limited-size LRU cache to bound memory and purge old entries on document updates. 3) Monitor effectiveness: log cache hits vs misses. Check that cached answers remain relevant and accurate. Use metrics like cache hit rate and the latency difference (cached response in, say, <50 ms vs fresh LLM call ~seconds) to quantify the benefit. Ensure that when the cache is bypassed (miss), the normal pipeline works as expected.
Concurrency & semaphore control (pipeline parallelism)	Flask (async handling) and vLLM GPU server	High concurrent load (multiple users or requests arriving together); vLLM supports continuous batching; known GPU throughput curve vs concurrency	Prevents overload and exploits batching. Allowing some concurrency keeps the GPU busy and lets vLLM batch requests for higher throughput – e.g. vLLM’s continuous batching can yield 8×–23× throughput gains under heavy load ￼. However, too many concurrent requests without limits can saturate memory or queue tasks such that P95 latency suffers. A controlled semaphore (max N concurrent LLM generations) helps balance throughput vs latency. Impact: at optimal concurrency, GPU utilization is high (90%+), tokens/sec throughput maximized, and per-request latency stays low (due to batching efficiency ￼). Also prevents VRAM exhaustion by not scheduling beyond capacity.	If max concurrency is set too low, we under-utilize the GPU (requests wait idle despite capacity, reducing throughput). If set too high, the GPU may context-switch excessively or run out of memory (in worst case vLLM will start evicting KV cache pages, hurting performance). Tuning is needed as the optimal N depends on request size distribution and GPU specs. Also, coordinating concurrency in Flask (CPU threads/async) with vLLM (GPU tasks) can be complex.	1) Determine safe concurrency: load-test the system by gradually increasing concurrent requests and monitoring latency and gpu_vram_peak_mb. Find the point where adding more concurrency yields diminishing returns or spikes latency. 2) Implement semaphore: in Flask, use an async semaphore or worker pool to limit the number of simultaneous requests sent to vLLM (others wait in queue). E.g., if optimal is ~4 simultaneous generations on a 16GB GPU, enforce that. 3) Verify under load: simulate 2× or 4× the expected QPS and ensure P95 latency remains acceptable and no GPU OOM occurs. Check logs for any vLLM warnings (e.g. memory page evictions). Also verify that the vLLM batching indeed occurred (it may log batch sizes; you can instrument it to print when multiple requests are combined).
Rate limiting & QoS (throttle excessive use)	API gateway or Flask middleware	Defined usage policies (per-user API call limits, total QPS ceiling); need to protect system from overload or expensive requests	Ensures stability and fair usage. By capping the request rate, we avoid traffic spikes that could blow out latency (especially important for P95). It also limits worst-case energy draw by preventing pathological burst loads. Impact: a steady-state latency distribution (no long tail from queueing endless requests), predictable resource usage (GPU/CPU won’t be driven beyond their sustainable throughput). In cost terms, it prevents unexpected bill spikes from a single user’s heavy load.	Over-throttling can degrade user experience (legitimate requests rejected). If not carefully configured, it might kick in too early (under-utilizing resources) or too late (not preventing internal overload). Requires distinguishing between users if multi-tenant – per-user limits vs global limit. Also, some queries are inherently heavy (long inputs) – might need separate limits on size to manage latency.	1) Define limits: e.g. maximum R requests per user per minute, and/or a global max of M concurrent requests overall. Also consider a limit on input size (document pages) to bound processing time. 2) Enforce in Flask: use a rate-limiting library or custom middleware to reject or queue requests that exceed the rate. Return a clear message (HTTP 429) if a user hits the cap. 3) Test scenarios: simulate a misbehaving client spamming requests – confirm that beyond a certain point, additional requests are delayed or dropped, and the system continues serving others with stable latency. Log whenever rate limiting is triggered (user ID and count) to see if limits are frequently hit (if so, may need adjustment or more capacity).
Batching with vLLM (continuous batching)	vLLM server – GPU scheduling policy	Multiple requests in flight; vLLM configured for continuous batching (default); possibly tune batching parameters (max tokens per batch, etc.)	Maximizes GPU efficiency by grouping token processing across requests. Continuous batching means the server dynamically forms batches as tokens are ready to generate ￼. This yields huge throughput gains: e.g., up to 23× higher throughput than naive per-request processing, while reducing median latency, as shown by Ray Serve + vLLM tests ￼. The GPU achieves better occupancy, leading to lower J/token (energy per token goes down when throughput is high). P95 latency under load improves because tasks don’t wait for completely free GPU – they share it. VRAM is used more uniformly (less fragmentation due to vLLM’s memory management).	If requests are extremely large (long prompts), they may dominate a batch and reduce opportunities to batch others (vLLM’s chunked prefill can mitigate this by breaking long prompts into chunks to intermix with other jobs ￼). Also, if traffic is very low (serial requests), you get no batching benefit – but vLLM’s scheduler will simply run them immediately, so only minimal overhead. A potential pitfall is that continuous batching could introduce slight, controlled delays (a few milliseconds) to wait for batchable work – usually negligible, but it means single-request latency might be slightly higher than a completely unbatched system at very low load.	1) Tune vLLM batching settings: check vLLM’s config (e.g., --max-batch-tokens or scheduling interval). Usually the defaults are good, but ensure no config is disabling continuous batching. You might set a small delay (e.g. 2–5 ms) to allow batch formation. 2) Send concurrent requests: use a load generator to send, say, 10 queries at once and observe the server. Verify via logs or metrics that vLLM is processing multiple requests together (e.g., log shows interleaved token outputs or an internal stat for “active requests in batch”). 3) Measure throughput/latency: compare with and without batching (e.g., run the same 10 queries sequentially vs concurrently). You should see overall throughput (tokens/sec) dramatically higher with concurrency, and average latency per query potentially lower when processed together. Track GPU utilization (should approach 90-100% during batch processing). Also verify correctness remains (batched execution should not affect output content).
Retry & backoff (robustness policy)	Flask – around external calls (embeddings, DB, LLM API if any)	Calls that can fail intermittently (e.g., network issues to Milvus, OpenAI API rate limits, SFTP timeouts); idempotent operations (safe to retry)	Increases reliability of the system. Rather than outright failing a user query when a sub-component fails, a modest retry with exponential backoff can recover without user impact. Impact on metrics: slight increase in P95 latency for the affected request (due to retry delay), but avoids total failure (which would be a “100% error latency”). Energy impact is minimal (a retry might repeat some work, but far better than user retrying the entire query). No effect on VRAM. Overall, better success rate = less wasted user and compute effort.	Too aggressive retries can amplify load (e.g., if Milvus is down and every request hammers it 5 times). Backoff is essential to avoid retry storms. Some failures are not transient (retrying won’t help), so identification is key. Also, each retry adds latency – after a couple of retries, the user might have waited too long anyway. Logging is needed to ensure we notice underlying issues rather than masking them with endless retries.	1) Identify retryable steps: e.g. embedding model call, Milvus query, SFTP fetch, optional calls to external LLMs. Implement try-catch for these in Flask. 2) Configure policy: for each, attempt perhaps 1 immediate retry, then a second retry after a short backoff (e.g., wait 0.5s then 2s). Use exponential backoff and limit total tries to prevent infinite loops. 3) Test failure modes: simulate a Milvus outage or timeout – confirm the system logs the failure, waits, and retries. The second attempt might succeed; if not, it should return a graceful error message to the user (instead of hanging or crashing). Monitor logs for repeated retries; if a specific component often requires retries, that’s a sign to investigate or increase its robustness. Verify that successful retries still yield correct final answers.
Provenance enforcement (source-grounded answers)	Prompt design (system prompt) and post-processing	Retrieved documents available; model capable of citing sources (fine-tuned or prompted to do so); evaluation process to validate sources	Improves faithfulness without large cost to efficiency. By instructing the LLM to include source attributions for facts (and possibly verifying them), we ensure the answer is grounded in retrieved data. Quality impact: higher factual accuracy and user trust. Efficiency impact: slight increase in tokens (the answer will include citations, e.g. document titles or numbers), which is negligible relative to the benefit of avoiding hallucinations. Energy/latency overhead is minimal (formatting citations costs few tokens). In our pipeline, retrieval is already done, so provenance enforcement just makes full use of it – the model’s attention is guided to use those snippets.	The LLM might still hallucinate a citation or attribute text to the wrong source if not properly constrained. It can also be overly verbose with sources if not instructed well (using up tokens). Ensuring every statement is backed by a source might make the answer longer or more hesitant. Post-verification (having another agent check each source) would add significant overhead – so we rely mainly on the prompt and model behavior.	1) Prompt tuning: in the system or user prompt to the R-4B model, explicitly say: “You are a QA system with document sources. Use the provided passages to answer, and cite each passage (e.g., [Source 1]) that supports your answer. Do not add information not found in sources.” Emphasize that unsupported claims are not allowed. 2) Answer format check: after generation, ensure the answer includes citations (e.g., look for the pattern of citation tokens 【...】). If any answer comes back without a source, have a policy to either regenerate with a stronger instruction or append a warning that the answer might be ungrounded. 3) Verify correctness: log the sources returned for each answer. During evaluation, manually verify a sample of answers: do the cited sources actually contain the facts stated? This can also be partially automated by an overlap check between answer sentences and source text. By enforcing citation presence and reviewing their accuracy, we maintain high faithfulness. Over time, this also provides a feedback loop – if the model often fails to cite, we adjust the prompt or fine-tune it on example QA pairs with sources.

State-of-the-Art Methods and Metrics (Task B)

Table 2. Notable SOTA methods for routing, planning, and context control in multi-LLM or RAG systems. Each entry summarizes the method, experimental setup, key metrics, improvements over baselines, code availability, and caveats (including failure modes like misclassification or latency spikes).

Method (Year)	Setup & Scenario	Quality & Efficiency Metrics	Δ vs. Baseline	Code?	Caveats / Failure Modes
FrugalGPT – Cascaded LLM Fallback (Chen et al., 2023) ￼ ￼	Budget-aware QA: Uses a chain of LLM API calls from cheapest to most expensive (e.g., GPT-J → GPT-3.5 → GPT-4) with a learned evaluator deciding if the answer is satisfactory or if escalation is needed ￼ ￼. Evaluated on multiple QA-like tasks (including finance, legal Q&A, etc.) with 12 different LLM APIs.	Quality: Matched GPT-4’s accuracy on benchmark queries. Cost: Achieved up to 98% cost reduction for the same accuracy ￼ ￼. In one setting, also improved accuracy by 4% over GPT-4 at equal cost ￼. Latency: Incurred additional delay only on queries that needed multiple hops (most answered by first model).	vs Single LLM: For the same accuracy as using only the best model (GPT-4), FrugalGPT cut cost by 59–98% across datasets ￼. vs Always GPT-4: At similar cost, got higher accuracy (+4%) by selectively using GPT-4 only when needed ￼.	Yes – open source (Stanford) ￼.	Some queries require multiple calls (worst-case latency increases when an answer fails validation and it calls the next model). The success depends on the quality evaluator – if it fails to detect a bad answer from a cheap model, the system might return a flawed answer without ever calling the higher-quality model. Tuning the confidence threshold is tricky (too strict -> calls expensive model too often; too lax -> accuracy drops).
MixLLM – Dynamic Multi-LLM Router (Zhang et al., 2025) ￼ ￼	RouterBench evaluation: Ensemble of open LLMs (including new Llama 3.1 models) serving streaming user queries. Uses a learned router with enhanced query embeddings and a latency penalty term to select the optimal model per query under a time constraint ￼ ￼. Also supports continual learning from feedback to adapt over time. Tested on RouterBench dataset and real query streams, with hardware simulating heavy load (latency penalty to avoid slow models overused).	Quality: Achieved 97.3% of GPT-4’s quality (on mixed tasks) ￼ – measured via accuracy and GPT-4 evaluation on answers. Cost: Used only 24.2% of GPT-4’s cost on average for those queries ￼. Latency: Avoided high-latency outliers by factoring response time into routing (met specified time budgets; exact P95 not given but no congestion even under load ￼). Throughput: Maintained high throughput with minimal quality loss.	vs OptLLM (prior best): OptLLM reached ~96.4% of GPT-4 quality at ~33% of cost; MixLLM improved quality slightly and cut relative cost by ~9 points further (24% vs 33%) ￼. vs Single GPT-4: Quality slightly lower (~3% gap) but cost ~4× cheaper per query. Also more consistent latency under load (where GPT-4 would queue or be slow).	Likely yes (the paper references extending RouterBench ￼; code or data expected via authors).	Relies on having multiple LLMs loaded – which needs memory and engineering overhead. The learned router can mispredict on out-of-distribution queries (sending a complex query to a small model, causing a bad answer). The system’s continual learning helps, but requires ongoing feedback data. If the latency penalty is mis-tuned, it might over-favor faster (but weaker) models, hurting quality, or vice-versa. Under extremely strict latency constraints, even the router might not find an adequate model in time (leading to degraded answers or none).
Performance-Aware Model Selection (Sakota et al., 2024) ￼	OpenAI API cost optimization: Evaluated routing among OpenAI’s GPT-3 series models (Ada, Babbage, Curie, Davinci) for QA tasks under a budget. They predict each model’s performance on a query via a learned regression and apply strategies: highest-score (best quality), threshold (cheaper model if above quality threshold), or ILP optimization to maximize quality under cost budget ￼ ￼. Benchmarked on knowledge tasks (MMLU, GSM8K, etc.).	Quality: Matching Davinci-002 (best model) accuracy in classification and coding tasks. Cost: With a cost-sensitive selection, they attained the same accuracy as always using Davinci at only 38% of the cost (~62% cost reduction) ￼ ￼. Budget use: One strategy achieved an 11% cost reduction with <1% accuracy loss (almost no quality drop) ￼. Latency: Not explicitly reported, but using smaller models for many queries presumably improved average latency (Ada is much faster than Davinci).	vs All Davinci: Saved ~62% cost with no accuracy drop by using smaller models when possible ￼. vs Random or always-small: Significantly higher accuracy by smart selection (comparable to best model’s accuracy, whereas always using Ada would severely drop accuracy).	Not sure (custom experiment; possibly code in a GitHub repo “routerbench” is referenced ￼).	Requires accurate performance prediction per query. Mis-estimation can pick a too-weak model, failing the query. They note needing to solve an ILP for budget allocation – not real-time friendly if many models. Also, their approach was tuned to specific OpenAI model set; it might not generalize to other model families without retraining. In production, API price changes or model deprecations could break the optimized strategy.
Adaptive-RAG – Complexity-Based Retrieval Routing (Jeong et al., 2024) ￼ ￼	Dynamic retrieval augmentation: Rather than one fixed RAG approach, they train a T5-Large classifier to categorize queries into three complexity levels (low: no retrieval needed, medium: one-step RAG, high: multi-step RAG) ￼. Systems evaluated: no retrieval (answer from parametric memory), standard single-step RAG (retrieve top-k once), multi-step (iteratively retrieve follow-up info). Tested on QA datasets requiring varying degrees of multi-hop reasoning.	Quality: Matched or improved answer accuracy by using more complex retrieval only for hard queries (the router “tends to favor more complex architectures” when needed) ￼. For easy queries, the no-retrieval path didn’t hurt accuracy on those. Latency: Overall latency remained on par with the single-step RAG baseline ￼ – because many queries were answered with minimal or no retrieval, offsetting the few that used multi-step (which is slower). Essentially, they achieved the benefit of multi-step RAG on hard queries without paying its cost on simple ones.	vs Naïve RAG (always single-step): Similar latency, but better accuracy on complex questions (since those got multi-step when needed) ￼. vs Always multi-step: Much faster on average (since easy questions skip that overhead) while giving similar accuracy; also fewer tokens fed to the LLM overall (saving cost/VRAM for simple queries).	Yes (GitHub repo available ￼).	Complexity classification is a hard problem itself – some “moderate” queries might be misclassified as low complexity, leading to missing needed retrieval (and an incorrect answer). They didn’t compare directly to a baseline prompt (e.g., maybe GPT-3.5 with no retrieval); if the retrieval steps are slow, a powerful LLM without retrieval might compete on latency. Also, their classifier favored complex methods, meaning it might over-trigger multi-step retrieval (which could waste time if a single step would do). In production, maintaining multiple RAG pipelines (and a graph of steps for multi-hop) increases system complexity.
LightRAG – Lightweight Graph-Based Retrieval (Peking Univ., 2024) ￼ ￼	Dual-level retrieval for RAG: Proposes a hybrid index: combine a global entity graph with local vector embeddings ￼. During ingestion, extracts entities and relations from text to build a knowledge graph (nodes = sentences & entities) ￼. At query time, first perform standard dense vector search to get relevant sentence nodes, then expand via graph edges to gather related context (neighboring facts) ￼. Tested on knowledge-intensive QA where GraphRAG (full graph retrieval) had shown quality gains at high computational cost.	Quality: Improved answer coherence and factuality over a naive vector-only RAG (the graph brings in broader context to answer multi-faceted queries) ￼ ￼. Users reported more complete answers for complex queries. Efficiency: Achieved this without the heavy cost of full graph algorithms – lower latency than prior GraphRAG approaches ￼ ￼. It reduces retrieval token overhead by not pulling in entire subgraphs, only pertinent paths. Memory/VRAM use is efficient: the graph index is lightweight (NetworkX based) and query-time graph traversal is fast.	vs GraphRAG (full graph retrieval): Faster – often lower latency since LightRAG prunes irrelevant parts of the graph and avoids massive prompts ￼. Only a minimal graph around relevant entities is added to the prompt. vs Standard RAG: More accurate on multi-hop questions (graph provides needed links) while keeping speed close to standard RAG. Overall, a better accuracy/latency trade-off than either extreme (pure vector or heavy graph).	Yes – released (Python package on PyPI, GitHub available; ~12k stars as of 2025) ￼ ￼.	Building and maintaining the graph index adds complexity: the ingestion pipeline must do entity extraction and graph updates whenever documents change. If the text is highly unstructured or the domain doesn’t have clear entities, the graph might not help much. A failure mode is retrieving a “broad but shallow” set of neighbors – if the graph connectivity is dense, the system might pull in loosely related info (risking distraction or mild hallucination). However, because it still relies on the LLM to synthesize, a hallucination with an authoritative-sounding citation is possible if an irrelevant node is included. Ensuring the graph remains up-to-date with the vector index (consistency) is also a challenge as data scales.

Synthesis: When Multi-Agent Routing Helps vs. Hurts (and Big-Model Fallbacks)

Adaptive routing and multi-agent orchestration shine when query diversity is high – in our pipeline, simple factoid questions can be handled by a small model or even directly from the vector index, whereas complex or ambiguous queries benefit from additional reasoning or a larger model. By routing each query to an appropriate “expert” (be it a specific tool, retrieval strategy, or a bigger LLM), we preserve quality while avoiding unnecessary costs ￼ ￼. This leads to lower average latency and energy per turn, since trivial requests don’t invoke heavy models or multi-step processes. However, such setups can backfire if the routing logic is flawed. Under a single 4B model, a mis-configured agent framework might add overhead (e.g. doing redundant analysis of the query) without actual gains – the 4B may lack the capacity to utilize complex plans, so extra steps just mean extra latency. In the worst case, a poor router sends a hard question to the 4B when it really needed a more powerful model, resulting in wrong answers and wasted cycles. Thus, the benefit of multi-agent routing hinges on reliable meta-decisions: the system must accurately detect query difficulty and know its tools’ limits. Adding a “big model” fallback (such as GPT-4 for the rare hardest queries) is generally worthwhile for a 4B-centric system – it provides an escape hatch for queries the 4B would otherwise mishandle. The trade-off is cost and a slight delay on those queries, but as long as the router correctly identifies only a small fraction for escalation, the overall user experience improves (most answers stay fast and cheap on the 4B, with a few slower but correct answers from the big model). In summary, routing and multi-agent strategies help when they are selectively and judiciously applied: they should engage added resources or steps only when a query’s complexity justifies it. When every query is treated the same, either all going through a complex chain or all through a small model, the advantages vanish – you either waste computation on easy questions or sacrifice accuracy on hard ones. The key is a well-tuned balance, using the right model/tool for the task at hand.

Open Research Gaps & Hypotheses for This Stack
	1.	Cache-aware Query Routing: Leverage the existing Milvus hybrid search + cache to decide route. Hypothesis: If a user query has a high semantic overlap with a recently answered question (detected via embedding similarity in Milvus or an exact cache hit), the system could skip a full LLM generation and reuse the cached answer or just lightly edit it. This “answer cache router” could drastically cut latency (perhaps by 50% for those repeated queries) and energy, while maintaining faithfulness – as long as we verify the cached answer’s sources still apply.
	2.	Conditional HyDE Generation: Integrate the HyDE step (Hypothetical Document Embedding) as a optional tool in routing. Under a 16GB VRAM budget, running even a small LLM for HyDE on every query is costly. Hypothesis: Train a classifier to detect when a query is too short or abstract for direct embedding retrieval – only then perform HyDE using the 4B model (or an even smaller language model). This could preserve accuracy on hard queries (by improving retrieval recall) without paying the token cost for HyDE on easy queries. We expect improved retrieval for “vague” queries (leading to higher answer recall), with maybe ~10% extra latency only on those cases, and no impact on simpler questions.
	3.	Graph-augmented RAG on Demand: Combine our vector DB with a lightweight graph like LightRAG for complex multi-hop queries. We can maintain a parallel NetworkX graph index of entities from documents (as LightRAG does) and route queries: if initial retrieval finds many disparate pieces or low-confidence results, trigger a graph-based expansion. Hypothesis: Using graph expansion only for the top 10% most complex queries will improve answer accuracy (particularly on multi-entity questions) while adding minimal latency for others. Essentially, easy questions get answered via normal Milvus lookup, whereas hard ones get a second-pass retrieval through the graph. This should boost factual F1 on multi-hop QA by a measurable margin (perhaps +5-10%) with only a small increase in P95 latency, as the graph step is used sparingly.
	4.	Memory-Constrained Big Model Fallback: Investigate strategies to include a larger model (e.g., 13B or GPT-3.5) on a 16GB GPU without persistent load. One idea is on-demand loading or quantization: the big model (quantized to 4-bit) could be loaded into GPU memory only when the router flags a query as high difficulty. Hypothesis: Although loading a model on the fly incurs overhead (~1-2 seconds), for low-frequency hard queries this is acceptable. We predict that 90%+ of queries can be handled by the resident 4B model, and the remaining ~10% incur an extra 2s to load and answer with a 13B model – resulting in far better accuracy on those outliers. This dynamic loading keeps VRAM usage low normally and avoids needing two GPUs. We will measure how often the fallback triggers and ensure the latency trade-off is worth the quality gain (a fallback should correct queries the 4B got wrong, raising overall accuracy of the system).
	5.	Cache-aware Batching and Scheduling: Extend vLLM’s batching by grouping similar requests. If multiple users ask questions around the same document or topic, the system could recognize this (e.g., via embedding clustering of pending queries) and batch them so that they share context in the KV cache. Hypothesis: This “semantic batch scheduling” could further increase throughput (more token reuse) and reduce per-query energy. For instance, if two queries hit the same document, processing them back-to-back might avoid re-loading that doc’s embeddings into the cache, cutting redundant compute. We need to verify if vLLM’s PagedAttention can exploit such locality – if so, intentionally ordering queued requests by topic might improve effective tokens/sec and J/token by a noticeable factor (perhaps 10-20% in scenarios with clustered query topics).

Reproducibility & Logging Notes

To rigorously benchmark and iterate on these ideas, detailed logging in both Flask and vLLM is essential:
	•	Flask-side Logging: For each request, log the router’s decision (which path or model was taken, and the classifier score or rationale). Also log token counts – how many tokens in the user question, retrieved context, and the LLM’s output. This helps calculate cost ($/1k tokens) and check if context budgeting works (e.g., are we truncating often?). Record the retrieval latency for each query (time spent in Milvus + any reranker), since retrieval time contributes to overall latency and CPU utilization. Mark whether any cache was hit: e.g., a boolean if the query was answered from the result cache, which would correspond to near-zero generation tokens and very low latency. Additionally, log timestamps for major steps (query received, retrieval done, LLM start, first token received, LLM finish) – from these we can compute end-to-end latency and specifically time-to-first-token (TTFT). If using the cascade fallback pattern, log whenever a second LLM is invoked (and the outcome), to assess how often and why fallback is happening.
	•	vLLM (GPU-side) Logging: Enable or add logging for batching events – e.g., when vLLM combines requests, log the batch size or how many requests/tokens were processed together. This will validate the batching efficiency under concurrency. Capture KV cache usage stats: vLLM provides metrics like gpu_vram_peak_mb and kv_pages_evicted. Logging these per interval or query can show if we’re nearing memory limits or evicting cache (which would hurt performance). We should also tap into any prefix cache hit information – for instance, how often were we able to reuse a prefix from a previous request (this might be in vLLM debug logs). For power measurements, integrate power sampling hooks: use NVIDIA’s NVML or PyTorch’s profiler to log GPU power draw during runs, and similarly log CPU power (if available, e.g., via RAPL) during retrieval. This can be downsampled (e.g., sample every 100ms) and tagged to queries. From this, metrics like Joules per token can be derived (since we know how many tokens were generated in that interval). Also track throughput (tokens/sec) on the GPU – vLLM might report this, or we calculate it from timestamps and token counts. By logging tokens/sec alongside GPU utilization %, we can identify if the GPU is the bottleneck or if it’s underfed. All these logs should be timestamped and ideally correlated via a request ID, so we can join Flask logs with vLLM logs for a full picture per query.

Finally, to facilitate analysis, consider logging structured JSON events (for easy parsing) with fields like: request_id, routed_model, num_input_tokens, num_context_tokens, num_output_tokens, retrieval_time_ms, generation_time_ms, ttft_ms, cache_hit=true/false, batch_size (if this request was batched), gpu_vram_peak_mb, gpu_power_j, cpu_power_j. These will directly feed into calculating the metrics of interest: GPU headroom (from VRAM peak vs physical memory, and if any KV eviction happened), P95 latency (from generation_time or e2e time across requests), tokens/sec and J/token (from counts and power), CPU vs GPU utilization (we can derive utilization from throughput and parallelism logs, and directly log CPU time if possible). Having these in logs ensures that when you run benchmarks, you can compute the stats you listed and identify how each pattern implementation affects them.

Key References and Relevance
	1.	Chen et al. (2023) – “FrugalGPT: How to use Large Language Models while Reducing Cost and Improving Performance.” Highly influential (Stanford research) introducing multi-LLM cascades. Demonstrated huge cost savings (~98%) with minimal quality loss by routing queries to cheaper models first ￼ ￼. Peer-reviewed at least as an arXiv preprint, widely discussed in industry (covered in deeplearning.ai ￼). Established a foundation for budget-conscious LLM applications.
	2.	Zhang et al. (2025) – “MixLLM: Dynamic Routing in Mixed Large Language Models.” NAACL 2025 long paper (peer-reviewed). State-of-the-art learned router that achieves near-GPT4 quality at a fraction of cost via a latency-aware model selection ￼. Notable for rigorous evaluation (extended RouterBench) and practical contributions (continuous learning, latency penalty). Illustrates cutting-edge academic work on multi-LLM orchestration.
	3.	Varangot-Reille et al. (2025) – “Doing More with Less – Routing Strategies in LLM-Based Systems” (Survey). Comprehensive survey of routing in conversational agents ￼ ￼, covering both model routing and routing at other pipeline stages. While an arXiv report (not yet peer-reviewed), it compiles and compares dozens of approaches (industrial and academic) and provides taxonomy and challenges ￼ ￼. Valuable for context and pointers to many specific methods (e.g., it summarizes results from Malekpour 2024, Stripelis 2024, Sakota 2024, etc., some of which we cited).
	4.	Jeong et al. (2024) – “Adaptive-RAG: Adaptive Retrieval-Augmented Generation.” (Conference TBD, GitHub available). Important for RAG-specific routing – it shows the benefit of switching retrieval strategies based on query complexity ￼ ￼. Provides a blueprint for complexity classifiers and measures the effect on latency and accuracy. Relevant as a peer-reviewed or at least well-documented research (possibly a workshop paper) with direct applicability to our RAG pipeline.
	5.	LightRAG (2024) – Graph-based Dual Retrieval Framework. Developed by Peking University (likely EMNLP 2024 or similar). Notable for combining knowledge graphs with vector search for efficiency ￼ ￼. Gained significant traction (open-source with thousands of stars) indicating replication and interest. While specific metrics are in the paper, secondary sources ￼ ￼ confirm it improves answer quality vs standard RAG without the heavy costs of prior GraphRAG – making it a state-of-the-art example in retrieval orchestration.
	6.	Sakota et al. (2024) – OpenAI Model Routing Strategies. (Possibly presented in a workshop or arXiv). This work is notable for using mathematical optimization (ILP) and threshold policies to choose among GPT-3 family models ￼ ￼. It’s relevant because it quantitatively showed large cost savings (~60%) at equal accuracy, using real OpenAI API costs – a practical scenario for many systems. The credibility is high given it builds on known models and was cited in the 2025 survey.
	7.	Shnitzer et al. (2023) – “Out-of-Distribution Confidence for Model Routing.” (Likely a refereed workshop paper). Introduced an approach with binary classifiers per model and an OOD detector to decide which model would perform best on an unseen task ￼ ￼. Significant because it aimed at robust routing even for queries unlike the training data, and reportedly could beat always using the largest model on average ￼ ￼. This highlights a failure mode (distribution shift) and a solution, lending depth to routing strategy considerations.
	8.	AnyScale Team (2023) – “Continuous Batching enables 23× Throughput…” (Blog). Not peer-reviewed, but by engineers (Daniel et al.) behind Ray Serve and vLLM ￼. Provided empirical evidence of the massive efficiency gains from advanced batching ￼. It’s relevant as a proof-of-concept in industry, influencing how we employ vLLM. The blog’s affiliation (Anyscale/Ray) and clear metrics make it a trustworthy source for systems optimization.
	9.	Paracha et al. (2025) – “Arch-Router: Aligning LLM Routing with Human Preferences.” Presented in mid-2025 (likely at a conference or on arXiv, with code on HuggingFace). Stands out by focusing on subjective quality – using a 1.5B router to incorporate user-defined domain/action preferences for routing ￼ ￼. While more about preference alignment than pure efficiency, it underscores an important point: routing success isn’t just about technical metrics, but also choosing models/tools that align with user expectations (e.g., style, domain). It’s a newer angle, indicating the evolving state-of-the-art where routing is not only automated but also customizable.
	10.	Ong et al. (2024) – “RouteLLM: Diverse Routing Strategies.” (Mentioned in MixLLM related work ￼, likely a 2024 publication). It introduced multiple routing approaches including similarity-based and learning-based methods. I include it for completeness: it signals that many variants were explored concurrently (classification, quality prediction, even reinforcement learning). While not individually groundbreaking, it represents the richness of techniques that were tested in the community by 2024, often in peer-reviewed venues (possibly NeurIPS workshops or ACL). It reinforces that our chosen strategies are built on a well-studied foundation.