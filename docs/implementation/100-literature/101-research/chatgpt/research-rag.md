Quality per Joule in RAG: Best Practices and SOTA Pipelines

Table 1: RAG Retrieval-Time Design Patterns (Quality vs. Efficiency)

Decision/Tactic	Typical Parameters/Settings	Effect on Quality (nDCG/EM/F1)	Effect on Efficiency (P95 Latency / VRAM / Joules)	Risks / Caveats	Quick A/B Test Plan (Flask + Milvus + BGE-M3 + vLLM)
Chunk size & stride (document segmentation)	e.g. 200â€“400 tokens per chunk with ~15% overlap vs. larger 500+ token chunks ï¿¼	Smaller chunks boost recall (more relevant pieces retrieved, raising nDCG) ï¿¼. Larger chunks carry more context for coherence but may dilute relevance. Too large chunks can miss matching a specific query detail, reducing recall/EM.	Smaller chunks = more total chunks to index/search (â†‘ index size, â†‘ retrieval compute) ï¿¼, which increases latency and memory. Larger chunks = fewer vectors (faster retrieval) but each retrieval result is longer (more tokens passed to LLM).	Overlap causes duplicate content in prompt; very small chunks may fragment context (LLM must piece them together). Very large chunks risk low recall if query-relevant info is buried in a long vector.	Index documents at two chunk sizes (e.g. 256 vs 512 tokens) and measure retrieval Recall@k or answer accuracy on a sample QA set. Track latency and GPU memory usage to identify optimal chunking that balances quality and speed.
Hybrid dense + sparse retrieval (BM25 + embeddings)	Combine vector similarity with BM25; e.g. retrieve top-10 from each, weight or merge results with tuned weight (Î»). ï¿¼	Often improves retrieval recall and answer accuracy for diverse queries by catching exact matches and semantic matches ï¿¼. Especially boosts precision on queries with rare keywords (BM25) while dense covers synonyms â€“ improving metrics like nDCG@k. If not tuned, hybrid can introduce off-target results (if one signal dominates).	Runs two searches instead of one â€“ higher latency and CPU cost (BM25 is CPU-bound) ï¿¼. Merging results adds minor overhead. Slightly higher energy use per query for the extra retrieval, though still often cheap relative to LLM inference.	Unnecessary complexity for simple queries where dense or BM25 alone suffices (adds ~2Ã— retrieval work with no quality gain). Requires tuning the weighting or ranking merge; a bad weight can hurt precision (e.g. overweighting BM25 may surface irrelevant exact matches).	Compare dense-only vs. hybrid on a set of queries that include some requiring exact term matches (e.g. code or proper nouns). Measure answer F1 or hit rate of the correct source in retrieved top-5. Note latency increase with hybrid. Adjust BM25 vs dense weight to see impact on precision/recall.
ANN index parameters (HNSW ef, IVF nprobe, quantization)	HNSW graph: ef_search 20 vs 100; IVF: nprobe 5 vs 50; PQ quantization levels.	Higher recall settings (larger ef or nprobe) retrieve more relevant docs (better Recall@k, hence higher nDCG/EM) ï¿¼. For instance, increasing nprobe broadens search and can ensure the answer-containing doc is found ï¿¼. Aggressive compression (PQ) can lower recall slightly, harming quality if important details are lost.	Higher recall = more comparisons (longer query time, more CPU/GPU work) ï¿¼. P95 latency rises as ef/nprobe increase. VRAM usage can rise if more index data is kept on GPU. Lower recall settings are faster/energy-saving but risk missing the correct passage (requiring costly fallback queries or leading to wrong answers).	Too-low recall (small ef/nprobe) can cause outright answer failure (relevant document not retrieved). Very high settings approach brute-force â€“ diminishing returns in quality but sharply increasing latency/Joules. PQ compression saves memory but if overdone can degrade embedding accuracy.	For a fixed set of queries with known relevant documents, vary HNSW ef or IVF nprobe. Measure Recall@10 and latency. Identify the smallest setting that still yields ~100% Recall@10. Likewise, compare an IVF-PQ index vs. IVF-FLAT to see if quality drops (and measure memory/latency trade-off).
Top-k retrieved & filtering (how many results, pre-filtering corpus)	Initial retrieval k=5 vs 30; apply metadata filters (e.g. domain or date) before search.	Increasing k can improve answer recall (more chances the correct info is included, helping EM/F1 until saturation) â€“ e.g. going from 5 to 20 documents can raise answer recall, especially for multi-hop questions. But beyond a point, additional documents yield diminishing quality and can even confuse the generator (irrelevant info). Applying filters (by source, date, etc.) can raise precision by excluding irrelevant chunks (boosting quality if filter is correct). ï¿¼ ï¿¼	Higher k directly increases work: more embeddings to compare and more text fed into the LLM (increasing prompt tokens and latency). Latency can grow linearly with k until hitting context length limits. Energy per query rises with k due to extra vector math and generation on longer prompts. Filters reduce workload by narrowing search scope ï¿¼, thus improving speed â€“ but only if the filtering itself isnâ€™t too costly and the index is partitioned efficiently.	If k is too low, relevant context might be missed (hurting accuracy). If k is too high, the LLM may receive many irrelevant pieces, risking distraction or contradiction (lower faithfulness) and wasting tokens. Filter misuse (wrong query classification) can exclude needed documents â€“ a major quality risk. Maintaining filters (metadata) adds complexity.	Run the pipeline with k=5, 15, and 30 on a benchmark (e.g.  NaturalQuestions). Measure answer EM/F1 and 95th percentile latency. Observe at what k quality plateaus vs latency. Also test a metadata filter (e.g. restrict to a specific collection for relevant queries) â€“ measure quality change for those queries and latency reduction from searching a smaller index.
HyDE query expansion (LLM-generated hypothetical document)	Use an LLM (e.g. GPT-4 or local 4B model) to generate a pseudo-answer, embed that for retrieval ï¿¼. Apply only if query is short or abstract vs. for all queries.	When applied to vague or zero-shot queries, HyDE can significantly improve retrieval effectiveness â€“ it â€œqueries with a narrativeâ€. This often yields higher recall and nDCG than using the raw query, outperforming unsupervised baselines (HyDE > Contriever and BM25 on nDCG@10) ï¿¼. It captures query intent, boosting downstream QA accuracy. However, if the LLM-written document goes off-topic or hallucinates, those errors can mislead retrieval ï¿¼. In cases where the query was already clear, HyDE offers little quality gain.	Non-trivial overhead: an LLM generation per query. Using a large model (GPT-3.5 as in the original HyDE) adds seconds of latency and considerable compute cost ï¿¼ ï¿¼. A smaller local model (e.g. the 4B generator already loaded) can be faster but may produce lower-quality expansions. VRAM: generating the hypothetical doc on the same GPU as the retriever/generator uses memory for the LLMâ€™s weights and KV cache. Overall, energy per query increases due to the extra LLM run.	If the LLM lacks knowledge on the topic, the hypothetical doc may be nonsense ï¿¼ â€“ retrieval might then bring back irrelevant results (hurting quality). Wasted effort if the original query was sufficient. Also, careful that the generated text isnâ€™t directly shown to the user (it may contain false info).	Configure HyDE to run only for certain queries (e.g. short questions or those with <X retrieval score). Evaluate on a dev set: compare answer accuracy with HyDE always-on vs. off. Note the latency hit. Also evaluate a conditional approach: only use HyDE when the initial dense retrieval confidence (max cosine similarity) is below a threshold â€“ see if this yields most of the quality gain with less average latency.
Reranker model & thresholds (cross-encoder vs. late interaction, when to rerank)	Options: a cross-encoder (e.g. miniLM or LLM-based) that scores top-$N$ passages vs. using a lighter ColBERT-style vector re-ranker. Use a confidence threshold to decide how many passages to rerank or whether to rerank at all.	A learned reranker can substantially improve precision of retrieved docs ï¿¼. Cross-encoders (which fully read query+passage) often yield the highest ranking quality â€“ boosting metrics like nDCG and final answer exact match by filtering out irrelevance. For example, a powerful reranker improved QA EM by a few points in HyperRAG (NQ EM ~26.8 â†’ 30.0) at rerank-20 ï¿¼ ï¿¼. ColBERT (late interaction) improves ranking too, though typically slightly less than a cross-encoder, in exchange for efficiency. Using a cutoff (e.g. only rerank top-20 and then take top-5, or drop low-scoring passages) maintains quality while avoiding feeding too much text to the generator ï¿¼. If initial retrieval scores are very confident, one might skip reranking entirely to save time (little quality loss in easy cases).	Cross-encoders are expensive â€“ they add an extra inference for each passage re-scored (batched, but still GPU-heavy). This can add hundreds of milliseconds or more, especially with larger rerankers ï¿¼. ColBERT-style rerankers pre-encode passages and do cheaper token-level matches, using more memory but less compute per query. Overall, reranking trades extra computation for potential quality gain. The thresholding can improve efficiency: e.g. if the reranker finds the top 5 have much higher scores than the rest, the pipeline can cut off and send fewer chunks to the LLM ï¿¼, saving token consumption. Systems like HyperRAG show that smart caching of rerank computations can reclaim much of this cost (2â€“3Ã— throughput gain by reusing reranker KV cache) ï¿¼.	Heavy rerankers demand significant VRAM (for the model or large embeddings) and can become a latency bottleneck under load ï¿¼. There is a point of diminishing returns: reranking lots of candidates yields minimal quality improvement but linearly adds cost. If the threshold for skipping rerank is too aggressive, you may skip it on queries that actually needed it (hurting accuracy). ColBERT requires maintaining multi-vector indexes, complicating the system.	Run the pipeline with no reranker vs. with a cross-encoder reranker (e.g. re-rank top-30 to choose best 5). Measure the improvement in answer correctness (EM/F1) and the added latency. Then experiment with a simpler reranker (e.g. BGE-M3â€™s own dot score as a baseline vs. a smaller cross-encoder) to see quality vs. speed trade-off. Test a cutoff: allow the reranker to prune any passage below a score threshold or only rerank if initial top-1 score is below X. Observe if latency drops without significant loss in answer quality.
Result caching (retrieval and answer cache)	Cache key: could be the normalized query text or embedding hash; store retrieved doc IDs and/or final answer. Cache TTL or invalidation on index update.	Caching doesnâ€™t improve one-shot quality directly, but ensures consistency and can avoid regressions. If the same question is asked repeatedly, a cache hit returns a previously generated answer (which presumably was correct if cached) â€“ effectively 100% precision for those repeats. It also enables using more expensive methods once and reusing the result. For instance, first query run with HyDE + rerank produces a correct answer, subsequent identical query can be served from cache even if those components are skipped â€“ quality stays high. Overall user experience can improve (more consistent answers, no quality variance due to randomness).	Huge win for efficiency on repeated queries: a cache hit bypasses embedding, ANN search, and even LLM generation if answer cached ï¿¼. This cuts latency to near-zero (just a lookup) and virtually no compute (greatly reducing mean energy per query in practice) ï¿¼. In a production setting with many recurring questions, this can reduce average load. VRAM: a cache might use some memory store but typically trivial compared to model size. The trade-off is storage (keeping embeddings or answers) and complexity of invalidation. If using an LLM answer cache, token costs ($) drop for repeated queries.	Limited benefit if queries are mostly unique (cache hit rate low, but overhead of checking cache is small). Must ensure cache invalidation when the knowledge source updates â€“ serving stale answers/content can harm trust. Answer caches risk giving outdated info if not carefully refreshed. Caching also needs careful key design (e.g. handling paraphrased queries â€“ those wonâ€™t hit exact cache even if answer would be same).	Identify a set of frequent queries (or simulate repeats in testing). Run the pipeline with caching enabled: measure latency for first vs subsequent occurrences of identical queries. Verify the answer is the same and still correct after skipping retrieval/LLM. Next, update a source document that a queryâ€™s answer depends on; ensure the cache is invalidated (the next query should do fresh retrieval). This test ensures the cache improves efficiency without sacrificing freshness.
Per-collection isolation (searching separate indexes by context/domain)	Maintain separate Milvus collections for different data sources or domains (e.g. internal docs vs public web). At query time, either route to the appropriate index (via classifier or user choice) or search both and merge results.	If the correct collection is queried, precision improves (no contamination from irrelevant domain data). This can boost quality on specialized queries â€“ e.g. a developer doc question finds the answer in the code docs index without distraction from web snippets. It also enables tuning embedding models per collection (improving relevance). When unsure of query domain and searching multiple indexes, quality can only improve (more coverage), but then it mimics a single big index result-wise. Wrong routing (query sent to the wrong index) can be disastrous for quality (zero relevant results retrieved).	Searching a smaller index (single relevant collection) is faster and uses less memory than one giant index â€“ lower latency and energy due to fewer vectors to scan ï¿¼. It also allows index loads to be selective (saving VRAM by not loading all data). However, if the system must search all collections in parallel (when unsure), it multiplies the retrieval work (e.g. 2 indexes = 2 searches, doubling latency/CPU load). Merging cross-index results adds minor overhead. Still, parallel search can be executed concurrently if resources allow. Memory-wise, keeping multiple indexes may allow some to stay on disk until needed.	Requires a reliable routing mechanism. A classifier to pick the index adds complexity and can misroute queries. If multiple indexes are searched, total cost increases (though each is smaller). Also, maintaining many indexes (especially if they use different embedding models) increases system complexity.	Using the existing public/private separation, test queries known to belong to one domain. Measure retrieval accuracy when searching only the correct collection vs searching the combined corpus. Also test the classifier: feed it mixed-domain queries to see if it routes correctly. For efficiency, measure query latency when hitting one index vs querying both indices in parallel. Verify that restricting to a smaller index indeed lowers latency without missing relevant results.
Provenance UX (displaying sources with the answer)	E.g. show source titles and a hyperlink or passage cite for each retrieved chunk used. Possibly highlight the most relevant snippet.	Not a direct algorithmic effect on metrics like F1, but it boosts perceived quality and trust. Users can verify facts against sources, which increases confidence in the answer. In practice, the requirement to show sources can indirectly enforce high faithfulness (the model is trained or prompted to ground answers in retrieved docs), reducing hallucinations â€“ thereby improving factual accuracy scores. If the answer is incorrect, showing sources makes it obvious, which is bad in the short term but good for transparency. Overall, provenance doesnâ€™t change the answer content (so EM stays same), but can improve qualitative evaluations of correctness and usefulness ï¿¼ ï¿¼.	Minor impact on efficiency. Formatting the citations and maybe fetching additional metadata (like page title) is a trivial overhead (milliseconds). The main cost is that the answer might be slightly longer (including source refs) â€“ a few extra tokens for the LLM to generate and for transmission. Energy or latency cost of those extra tokens is negligible compared to the rest. In fact, having provenance could avoid follow-up questions or user second-guessing, indirectly saving compute. VRAM overhead is none (just uses existing data).	If not done carefully, could expose model mistakes â€“ e.g. if the modelâ€™s statement isnâ€™t actually supported by the cited source, the user will catch the discrepancy (eroding trust). Thereâ€™s also UX risk: too many sources or overly technical citations might confuse users. From a system perspective, thereâ€™s little downside except the development effort to implement it.	Conduct a user study: have users ask the system questions, some get answers with sources, others without. Collect feedback on trust and satisfaction. Also internally track if showing sources changes user behavior (do they accept answers more readily, or catch errors). In the pipeline, ensure that the tokens spent on citing sources donâ€™t push prompts over length limits â€“ test an extremely long answer with sources to see if truncation occurs.

Sources are preserved in citations above for verification of these points.

Table 2: State-of-the-Art RAG Systems â€“ Quality vs. Efficiency Metrics

System (Publication)	Datasets / Tasks	Key Settings (Retriever â†’ Generator)	Quality Metrics (Retrieval & QA)	Efficiency Metrics (Latency, VRAM, Energy, Cost)	Gains vs. Baseline	Code?	Caveats / Notes
HyperRAG (CMU/UChicago 2024) ï¿¼ ï¿¼	Open-domain QA (TriviaQA, NQ, PopQA) â€“ passage & doc corpora	Dense retriever (Contriever); uses LLM reranker (Gemma-2B) on top-$N$ (5â€“20) â†’ Llama3.1-8B generator, with KV cache reuse for reranker ï¿¼.	EM on QA: e.g. NQ ~30.0 EM with reranker (vs ~26.8 without) ï¿¼; TriviaQA ~59.6 EM. Reranker improves relevance (higher nDCG) and yields more grounded answers. High faithfulness due to strong reranking.	Throughput: ~28.9 QPS with reranker@$N$=20 using KV-cache vs. ~13.3 QPS without reuse (2.2Ã— boost) ï¿¼. P95 latency well under 1s for 5-passages reranked, scales with $N$. VRAM: needs extra ~40TB storage for KV caches in worst case (for large corpora) ï¿¼; runtime GPU memory dominated by 8B generator & reranker models.	Quality +: Outperforms no-rerank baseline by +3â€“4 EM on QA ï¿¼. Efficiency +: 2â€“3Ã— throughput vs. naive reranker approach (same quality) ï¿¼. vs. baseline (no reranker): achieves higher accuracy with only moderate latency increase (KV reuse avoids a full 5Ã— slowdown that a naive reranker would incur).	ðŸŸ¡ (Planned) â€“ Paper proposes method; code not public yet (to our knowledge).	Needs massive disk for cache (storing reranker KV for entire corpus) ï¿¼. Applicable when you can amortize setup cost over many queries. Complex system integration (KV-cache management).
LightRAG (EMNLP 2025) ï¿¼ ï¿¼	Multi-domain academic QA (UltraDomain benchmark: Agri, CS, Legal, etc.)	Dual retrieval: embedding index + a knowledge graph of entities. Graph-enhanced retriever finds connected relevant chunks (low-level and high-level) â†’ LLM (e.g. ChatGLM) generator. Incremental graph updates for new data.	Win rate vs baselines: Beats naive dense RAG, HyDE, and prior GraphRAG on ~80% of queries in complex domains ï¿¼. Handles inter-dependent questions better (coherent, multi-entity answers). Higher retrieval accuracy (captures  relationships missed by flat retrieval). Human eval shows more comprehensive and correct answers for multi-hop queries (especially Legal domain where baselines <20% win) ï¿¼.	Latency: Improved response times over prior graph-based methods (LightRAG is optimized for speed) ï¿¼. Exact ms not given, but design avoids costly rebuilds. Scalability: Incremental update algorithm means new docs integrate cheaply â€“ e.g. adding a dataset doesnâ€™t require full re-index (GraphRAG required costly reprocessing ~O(corpus size)) ï¿¼. VRAM: running graph + vector search is heavier than vector alone, but still uses single GPU; memory trade-off for storing graph adjacency.	Quality +: Significant accuracy gains on complex queries vs. strong baseline GraphRAG (especially on largest dataset, LightRAG maintains high accuracy while others drop) ï¿¼ ï¿¼. Efficiency +: Faster than earlier GraphRAG (no expensive re-index for updates; more efficient retrieval). vs. baseline: LightRAGâ€™s dual retrieval yields more precise answers with comparable or lower latency than a standard RAG on these tasks.	ðŸŸ¢ Yes ï¿¼ (Open-source on GitHub).	More complex indexing (needs building and maintaining a knowledge graph). Overhead might not pay off for simple factual queries. Best suited when queries involve relational reasoning. Initial graph construction can be time-consuming.
VectorLiteRAG (ArXiv 2025) ï¿¼ ï¿¼	Open-domain and internal QA (evaluated on Wikipedia (Wiki-All) and ORCAS search logs)	Hybrid index partitioning: An IVF vector index is split between GPU and CPU tiers based on query frequency (hot clusters on GPU) ï¿¼ ï¿¼. Retrieval uses GPU for popular vectors and CPU for others, optimizing latency. Generator: tested with small (e.g. 7B) and large LLMs; retrieval on GPU shares resources with generation.	Maintains same answer quality as a normal RAG (no retrieval accuracy loss reported) ï¿¼ â€“ itâ€™s an infrastructure optimization. Recall@k essentially equal to baseline (since index algorithm is unchanged, just divided). Downstream answer correctness unchanged (ensures â€œno compromiseâ€ on generation quality) ï¿¼.	Latency/Throughput: Achieves up to 1.5Ã— higher throughput at p95 latency SLO (e.g. can handle 1.5Ã— query load while keeping <800ms) ï¿¼. At low traffic, GPU-accelerated search is ~10Ã— faster than CPU search ï¿¼ ï¿¼. Under high QPS, it outperforms pure GPU or pure CPU retrieval by intelligently load-balancing. VRAM: uses a portion of GPU for index â€“ careful tuning needed so it doesnâ€™t starve the LLM of memory ï¿¼ ï¿¼. Energy: by avoiding GPU overuse for cold data, it improves perf/Joule â€“ fewer GPU cycles wasted on infrequent queries.	Quality =: (Neutral) â€“ matches baseline accuracy. Efficiency +: ~50% more queries served under same latency budget vs baseline (which might be CPU-only or naive GPU sharing) ï¿¼. vs. Heterogeneous baseline: Unlike prior HeterRAG PIM hardware which helped at low load but struggled as concurrency rose ï¿¼, VectorLiteRAG sustains latency at scale by adaptive partitioning.	ðŸ”´ No (Research prototype; methodology in paper).	Complex deployment â€“ requires profiling query patterns and dynamic routing of vectors. Needs tuning per workload. Gains are notable when index is large and query distribution is skewed (hot versus long-tail queries); less impact if corpus is small or uniform.
â€œAtlasâ€ (Meta 2022) ï¿¼ ï¿¼	Open-domain QA (NaturalQuestions, TriviaQA, WebQ)	Dense retriever (HF Contriever) â†’ Fusion-in-Decoder generator (T5-based, 11B parameters) fine-tuned together (â€œAtlasâ€). No reranker; generator attends to top-40 passages directly.	Top-tier QA accuracy: e.g. NQ ~50% EM, TriviaQA ~68% EM, setting new state of the art in 2022. Outperforms a 175B GPT-3 on many QA tasks ï¿¼, despite far fewer parameters, thanks to retrieval. Very high recall â€“ 95%+ of answers found in retrieved set. Strong faithfulness due to grounding in passages.	Efficiency: Uses an 11B model instead of a 175B LM â€“ much lower VRAM and compute per query (roughly 15Ã— fewer FLOPs than GPT-3 for similar quality). Still requires ~40 passages per query Ã— 11B model inference â€“ inference is moderate (~a few seconds on one GPU). No explicit latency published, but clearly more efficient than a giant LM hallucinating answers. Cost: significantly cheaper to deploy than GPT-3 for comparable results (fewer tokens needed since answers are found more directly).	Quality +: Achieved SOTA accuracy with an order-of-magnitude smaller model by leveraging retrieval (vs. baseline GPT-3 or non-retrieval models). Efficiency +: Greatly reduced model size (11B vs 100+B) for same quality â€“ implies lower memory and $$ per query. vs. baseline DPR+BART (original RAG 2020): Atlas improved EM by ~5-10 points on QA benchmarks with similar model size, by joint training retriever and generator.	ðŸŸ¡ Partial (Code for retriever and model checkpoints available, but full pipeline integration not one-click).	Needs task-specific fine-tuning and large curated corpus. Memory intensive when handling 40 passages at once in the generator. Latency can add up because the generator attends to a lot of text (but still faster than a giant LM).

Sources: The above comparisons are drawn from connected papers and reports ï¿¼ ï¿¼ ï¿¼ ï¿¼ ï¿¼. Atlas results from Meta AI show the power of retrieval to use smaller models ï¿¼. All systems were evaluated on comparable hardware (e.g. A100 GPUs or similar), though exact energy usage (J/query) is seldom directly reported â€“ we rely on throughput and model size as proxies. Code availability is noted where applicable.

Synthesis (150 words)

HyDE vs. Rerank under tight VRAM: When GPU memory is limited, loading a large cross-encoder reranker may be impractical. HyDE offers an alternative by using the existing generator model to expand queries. HyDE improves retrieval recall without an extra model loaded ï¿¼, making it attractive in low-VRAM settings. It adds some latency for generation, but avoids storing a separate reranker network. A cross-encoder reranker, in contrast, demands dedicated VRAM but yields higher precision ï¿¼. Thus, with tight VRAM, one might favor HyDE (to boost recall) and rely on the generator to filter context, accepting a slight quality hit compared to a powerful reranker.

Hybrid vs. Dense-Only: Dense retrieval alone is faster and simpler â€“ all on GPU, and it handles semantic queries well. However, hybrid retrieval (dense + BM25) can markedly improve results for queries with exact keywords or rare terms ï¿¼. In our stack, hybrid search is worth the extra CPU cost when queries include critical keywords (e.g. error codes, proper nouns) the dense model might miss. If latency is paramount or queries are often short/abstract, dense-only is usually sufficient (and uses less energy by skipping the BM25 step). But for diverse real-world queries, a hybrid approach boosts quality, yielding higher answer success rates at a modest efficiency cost.

LightRAG (graph-based) overhead: LightRAG introduces a knowledge graph on top of embeddings to capture relationships. This overhead is justified when queries involve multi-hop reasoning or connecting facts across documents â€“ scenarios where graph context prevents fragmented answers ï¿¼. In such cases, LightRAG significantly improves answer completeness and correctness, so the extra indexing and retrieval work pay off. For our pipeline, if users ask complex analytical questions (e.g. how one entity influences another in a domain), the graph augmentation could be worth its maintenance cost. Conversely, for straightforward Q&A or small corpora, LightRAGâ€™s graph construction and traversal overhead might not be worthwhile â€“ a standard vector search is usually enough, and the simpler pipeline will be faster and easier to maintain.

Gap Hypotheses for Pipeline Improvement
	1.	Energy-aware hybrid retrieval weighting: Dynamically adjust the balance between dense and sparse retrieval based on runtime cost constraints. For example, if the GPU is underutilized but CPU is saturated (or vice versa), shift the weighting (or even disable BM25) to maximize quality per Joule. This could involve a controller that monitors query load and toggles hybrid components to meet an energy budget.
	2.	Conditional HyDE invocation via query confidence: Implement a lightweight query classifier or uncertainty estimator to decide when to use HyDE. If the initial retrieval has low confidence (e.g. no scores above a threshold or the query is highly novel), then trigger HyDE expansion; otherwise, skip it. This policy would save compute on easy queries while still reaping HyDEâ€™s benefits on hard ones â€“ improving average latency/energy usage without sacrificing answer recall.
	3.	Reranker-on-cache-miss (adaptive reranking): Instead of always reranking, use caching and query similarity to bypass the reranker when possible. If a new query is very similar to a previous one (cache hit on retrieved docs), reuse the prior reranker output or final answer. Only run the expensive reranker if the query is truly new or if the cached retrieval results led to a wrong answer last time. This could drastically cut redundant rerank computations, saving GPU cycles.
	4.	Graph-assisted retrieval on-demand: Integrate LightRAG in a toggleable way â€“ e.g., a â€œgraph modeâ€ that activates only for identified multi-hop questions. A router model could predict if a query would benefit from graph-based retrieval (complex question with multiple entities) and switch to LightRAG for that query, otherwise using standard vector search. This conditional use of graph augmentation would limit the overhead to cases where itâ€™s likely to boost quality, improving overall efficiency.
	5.	Adaptive chunk and context compression: Introduce a module that prunes or compresses context passages fed to the LLM based on their predicted relevance contribution. For instance, after retrieval (and optional rerank), if there are 10 chunks but the model can achieve a similar answer with 3â€“5 (perhaps as indicated by reranker scores or content overlap), skip feeding the rest. By reducing prompt length dynamically, the system would save tokens and energy, especially for questions where only a few passages are truly relevant. This ties into an energy-optimal use of the context window â€“ avoiding waste on low-impact text.

Reference Relevance (1â€“10 scale)
	â€¢	HyperRAG (Y. An et al., 2024) ï¿¼ ï¿¼ â€“ Relevance: 8/10. A recent academic work (CMU/UChicago) optimizing RAG rerankers. Not peer-reviewed yet (arXiv pre-print) but by credible researchers. High relevance for quality-efficiency trade-offs, demonstrating novel techniques (KV-cache reuse) with strong empirical results.
	â€¢	LightRAG (Z. Guo et al., EMNLP 2025) ï¿¼ ï¿¼ â€“ Relevance: 9/10. Peer-reviewed conference paper incorporating knowledge graphs into RAG. High importance due to its novel approach and significant quality gains on complex QA. Open-sourced, indicating practical impact. EMNLP is a top NLP venue, lending it credibility. Highly relevant for scenarios requiring structured knowledge in RAG.
	â€¢	VectorLiteRAG (J. Kim et al., 2025) ï¿¼ ï¿¼ â€“ Relevance: 8/10. Cutting-edge systems research (likely under review) focused on latency/throughput optimization. Offers solid experimental evidence on GPU/CPU partitioning. Not yet peer-reviewed, but addresses real deployment constraints (from NVIDIA researchers, judging by content). Relevant for engineering efficiency in large-scale RAG.
	â€¢	Precise Zero-Shot Dense Retrieval (HyDE, Gao et al., 2022) ï¿¼ ï¿¼ â€“ Relevance: 7/10. Well-known IR paper (arXiv and possibly SIGIR workshop) introducing HyDE. High technical merit for zero-shot retrieval improvement. Itâ€™s not in a top-tier conference but has influenced many practical RAG implementations (including Haystack, etc.). Good for understanding query expansion trade-offs.
	â€¢	HeterRAG (Liu et al., ISCA 2025) â€“ Relevance: 9/10. Top-tier computer architecture conference paper (ISCA) demonstrating specialized hardware acceleration for RAG via Processing-in-Memory. Although hardware-specific, it provides strong evidence on efficiency limits and innovations. Important for anyone considering dedicated infrastructure to speed up RAG.
	â€¢	Compute Hivenet RAG Blog (Oct 2025) ï¿¼ ï¿¼ â€“ Relevance: 6/10. Industry blog (not peer-reviewed) but written by practitioners highlighting performance best practices. Valuable for its practical tips (chunking, caching, etc.) with some empirical basis. Less authoritative than formal research, yet quite relevant for implementation guidance in our context.
	â€¢	Zilliz â€œImproving RAG with HyDEâ€ Blog (2024) ï¿¼ ï¿¼ â€“ Relevance: 5/10. Technical blog from Milvus creators. Explains HyDE and reports its benefits and limitations clearly. While not academically rigorous, itâ€™s useful for integration perspective (especially since our stack uses Milvus and BGE-M3). Medium relevance â€“ good explanatory resource with some quantitative notes.
	â€¢	Neptune.ai RAG Evaluation Guide (2025) ï¿¼ ï¿¼ â€“ Relevance: 6/10. An LLMOps-focused article on how to evaluate RAG across performance, cost, and latency. Not a research paper, but provides a framework for thinking about trade-offs. Helpful for ensuring we cover all dimensions of quality and efficiency. Moderately relevant (broad overview rather than specific to our design).

(The scores reflect general importance and credibility, prioritizing peer-reviewed work and novel contributions. Higher scores were given to sources with strong empirical results, prestige venues, or direct applicability to optimizing our RAG pipeline.)